"use strict";(self.webpackChunkmy_portfolio=self.webpackChunkmy_portfolio||[]).push([[965],{3905:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>d});var o=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function s(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?s(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):s(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},s=Object.keys(e);for(o=0;o<s.length;o++)n=s[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(o=0;o<s.length;o++)n=s[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=o.createContext({}),p=function(e){var t=o.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},m=function(e){var t=p(e.components);return o.createElement(l.Provider,{value:t},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},h=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,s=e.originalType,l=e.parentName,m=i(e,["components","mdxType","originalType","parentName"]),c=p(n),h=a,d=c["".concat(l,".").concat(h)]||c[h]||u[h]||s;return n?o.createElement(d,r(r({ref:t},m),{},{components:n})):o.createElement(d,r({ref:t},m))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var s=n.length,r=new Array(s);r[0]=h;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i[c]="string"==typeof e?e:a,r[1]=i;for(var p=2;p<s;p++)r[p]=n[p];return o.createElement.apply(null,r)}return o.createElement.apply(null,n)}h.displayName="MDXCreateElement"},5325:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>s,metadata:()=>i,toc:()=>p});var o=n(7462),a=(n(7294),n(3905));const s={},r="How to Format Inputs to ChatGPT",i={unversionedId:"tutorial-extras/copy",id:"tutorial-extras/copy",title:"How to Format Inputs to ChatGPT",description:"The orginal article, How to Format Inputs to ChatGPT, can be viewed on openai's GitHub repository.",source:"@site/docs/tutorial-extras/copy.md",sourceDirName:"tutorial-extras",slug:"/tutorial-extras/copy",permalink:"/docs/tutorial-extras/copy",draft:!1,editUrl:"https://github.com/CharitySarah09/CharitySarah09.github.io/tree/main/packages/create-docusaurus/templates/shared/docs/tutorial-extras/copy.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Editing Samples",permalink:"/docs/category/editing-samples"},next:{title:"Techniques for Improving Model Reliability",permalink:"/docs/tutorial-extras/increasing-reliability"}},l={},p=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Elements of a Chat API Call",id:"elements-of-a-chat-api-call",level:2},{value:"Input",id:"input",level:3},{value:"Output",id:"output",level:3},{value:"Example Chat API Calls",id:"example-chat-api-calls",level:3},{value:"Conversation-based task",id:"conversation-based-task",level:4},{value:"Nonconversation-based task",id:"nonconversation-based-task",level:4},{value:"Methods for Instructing gpt-3.5 turbo-0301",id:"methods-for-instructing-gpt-35-turbo-0301",level:2},{value:"System Messages",id:"system-messages",level:3},{value:"Few-shot Prompting",id:"few-shot-prompting",level:3},{value:"Counting tokens",id:"counting-tokens",level:2},{value:"Function for Calculating Tokens",id:"function-for-calculating-tokens",level:3}],m={toc:p},c="wrapper";function u(e){let{components:t,...n}=e;return(0,a.kt)(c,(0,o.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"how-to-format-inputs-to-chatgpt"},"How to Format Inputs to ChatGPT"),(0,a.kt)("admonition",{type:"tip"},(0,a.kt)("p",{parentName:"admonition"},"The orginal article, ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb"},"How to Format Inputs to ChatGPT"),", can be viewed on openai's GitHub repository.")),(0,a.kt)("p",null,"ChatGPT is powered by OpenAI's most advanced model, gpt-3.5-turbo. The chat models take series of messages as input, and then return an AI-written message as output. Users can use this software to build their own applications. This guide illustrates the chat format with examples of API calls."),(0,a.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,a.kt)("p",null,"OpenAI Python Library must be installed. "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},"# if needed, install and/or upgrade to the latest version of the OpenAI Python library\n%pip install --upgrade openai\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},"# import the OpenAI Python library for calling the OpenAI API\nimport openai\n")),(0,a.kt)("h2",{id:"elements-of-a-chat-api-call"},"Elements of a Chat API Call"),(0,a.kt)("h3",{id:"input"},"Input"),(0,a.kt)("p",null,"A chat API call has two required inputs:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"model"),": the model being used (e.g. gpt-3.5.turbo)"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"messages"),": the list of message objects; each object has at least two of the following fields",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"role"),": either system, user or assistant"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"content"),": the desired output (e.g. write me a beautiful poem)")))),(0,a.kt)("p",null,"A conversation usually begins with a system message followed by alternating user and assistant messages, but this format is not required."),(0,a.kt)("h3",{id:"output"},"Output"),(0,a.kt)("p",null,"A response object has several fields:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"id"),": the ",(0,a.kt)("em",{parentName:"li"},"ID")," of the request"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"object"),": the type of object returned (e.g. chat.completion)"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"created"),": the request's timestamp"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"model"),": the full name of the model used to generate the response"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"usage"),": the total number of tokens used to generate the replies, including those required for the prompt"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"choices"),": a list of completion objects",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"message"),": the message object generated by the model which includes the role and content"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"finish_reason"),": the reason the model stopped generating the text (either the stop, or lenth if max_tokens limit was reached)"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"index"),": the index of the completion in the list of choices")))),(0,a.kt)("h3",{id:"example-chat-api-calls"},"Example Chat API Calls"),(0,a.kt)("h4",{id:"conversation-based-task"},"Conversation-based task"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},'# Example OpenAI Python library request\nMODEL = "gpt-3.5-turbo"\nresponse = openai.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {"role": "system", "content": "You are a helpful assistant."},\n        {"role": "user", "content": "Knock knock."},\n        {"role": "assistant", "content": "Who\'s there?"},\n        {"role": "user", "content": "Orange."},\n    ],\n    temperature=0,\n)\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},'response\n<OpenAIObject chat.completion id=chatcmpl-6pjrV9CvZ2ivOSxzZrBdEidUB6xfs at 0x13362cf90> JSON: {\n  "choices": [\n    {\n      "finish_reason": "stop",\n      "index": 0,\n      "message": {\n        "content": "Orange who?",\n        "role": "assistant"\n      }\n    }\n  ],\n  "created": 1677789041,\n  "id": "chatcmpl-6pjrV9CvZ2ivOSxzZrBdEidUB6xfs",\n  "model": "gpt-3.5-turbo-0301",\n  "object": "chat.completion",\n  "usage": {\n    "completion_tokens": 5,\n    "prompt_tokens": 38,\n    "total_tokens": 43\n  }\n}\n')),(0,a.kt)("p",null,"To extract the reply only, use the following code:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},"    response['choices'][0]['message']['content']\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},"    'Orange who?'\n")),(0,a.kt)("hr",null),(0,a.kt)("h4",{id:"nonconversation-based-task"},"Nonconversation-based task"),(0,a.kt)("p",null,"To format non-conversation based tasks for chat API, place the instructions in the first user message. "),(0,a.kt)("p",null,"In the example below, the model is asked to explain asynchronous programming in the style of the pirate, Blackbeard."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},'# example with a system message\nresponse = openai.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {"role": "system", "content": "You are a helpful assistant."},\n        {"role": "user", "content": "Explain asynchronous programming in the style of the pirate Blackbeard."},\n    ],\n    temperature=0,\n)\n\nprint(response[\'choices\'][0][\'message\'][\'content\'])\n    \n')),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"Ahoy matey! Asynchronous programming be like havin' a crew o' pirates workin' on different tasks at the same time. Ye see, instead o' waitin' for one task to be completed before startin' the next, we can have multiple tasks runnin' at once. It be like havin' me crew hoistin' the sails while others be swabbin' the deck and loadin' the cannons. Each task be workin' independently, but they all be contributin' to the overall success o' the ship. And just like how me crew communicates with each other to make sure everything be runnin' smoothly, asynchronous programming uses callbacks and promises to coordinate the different tasks and make sure they all be finished in the right order. Arrr, it be a powerful tool for any programmer lookin' to optimize their code and make it run faster.")),(0,a.kt)("hr",null),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},'# example without a system message\nresponse = openai.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {"role": "user", "content": "Explain asynchronous programming in the style of the pirate Blackbeard."},\n    ],\n    temperature=0,\n)\n\nprint(response[\'choices\'][0][\'message\'][\'content\'])\n\n')),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"Ahoy mateys! Let me tell ye about asynchronous programming, arrr! "),(0,a.kt)("p",{parentName:"blockquote"},"Ye see, in the world of programming, sometimes we need to wait for things to happen before we can move on to the next task. But with asynchronous programming, we can keep working on other tasks while we wait for those things to happen. "),(0,a.kt)("p",{parentName:"blockquote"},"It's like when we're sailing the high seas and we need to wait for the wind to change direction. We don't just sit there twiddling our thumbs, do we? No, we keep busy with other tasks like repairing the ship or checking the maps. "),(0,a.kt)("p",{parentName:"blockquote"},"In programming, we use something called callbacks or promises to keep track of those things we're waiting for. And while we wait for those callbacks or promises to be fulfilled, we can keep working on other parts of our code. "),(0,a.kt)("p",{parentName:"blockquote"},"So, me hearties, asynchronous programming is like being a pirate on the high seas, always ready to tackle the next task while we wait for the winds to change. Arrr!")),(0,a.kt)("h2",{id:"methods-for-instructing-gpt-35-turbo-0301"},"Methods for Instructing gpt-3.5 turbo-0301"),(0,a.kt)("p",null,"The following applies to model instructions for gpt-3.5-turbo-0301. Keep in mind, the best practices may or may not work with future models as it can change with different versions."),(0,a.kt)("h3",{id:"system-messages"},"System Messages"),(0,a.kt)("p",null,"The assistant can be primed with different personalities or behaviours using the system message. However, the system message has comparatively less influence on output so more important instructions should be placed in the user message instead."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},'\n# An example of a system message that primes the assistant to explain concepts in great depth\nresponse = openai.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {"role": "system", "content": "You are a friendly and helpful teaching assistant. You explain concepts in great depth using simple terms, and you give examples to help people learn. At the end of each explanation, you ask a question to check for understanding"},\n        {"role": "user", "content": "Can you explain how fractions work?"},\n    ],\n    temperature=0,\n)\n\nprint(response["choices"][0]["message"]["content"])\n\n')),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"Sure! Fractions are a way of representing a part of a whole. The top number of a fraction is called the numerator, and it represents how many parts of the whole we are talking about. The bottom number is called the denominator, and it represents how many equal parts the whole is divided into."),(0,a.kt)("p",{parentName:"blockquote"},"For example, if we have a pizza that is divided into 8 equal slices, and we take 3 slices, we can represent that as the fraction 3/8. The numerator is 3 because we took 3 slices, and the denominator is 8 because the pizza was divided into 8 slices."),(0,a.kt)("p",{parentName:"blockquote"},"To add or subtract fractions, we need to have a common denominator. This means that the denominators of the fractions need to be the same. To do this, we can find the least common multiple (LCM) of the denominators and then convert each fraction to an equivalent fraction with the LCM as the denominator."),(0,a.kt)("p",{parentName:"blockquote"},"To multiply fractions, we simply multiply the numerators together and the denominators together. To divide fractions, we multiply the first fraction by the reciprocal of the second fraction (flip the second fraction upside down)."),(0,a.kt)("p",{parentName:"blockquote"},"Now, here's a question to check for understanding: If we have a pizza that is divided into 12 equal slices, and we take 4 slices, what is the fraction that represents how much of the pizza we took?")),(0,a.kt)("hr",null),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},'# An example of a system message that primes the assistant to give brief, to-the-point answers\nresponse = openai.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {"role": "system", "content": "You are a laconic assistant. You reply with brief, to-the-point answers with no elaboration."},\n        {"role": "user", "content": "Can you explain how fractions work?"},\n    ],\n    temperature=0,\n)\n\nprint(response["choices"][0]["message"]["content"])\n\n')),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"Fractions represent a part of a whole. They consist of a numerator (top number) and a denominator (bottom number) separated by a line. The numerator represents how many parts of the whole are being considered, while the denominator represents the total number of equal parts that make up the whole.")),(0,a.kt)("h3",{id:"few-shot-prompting"},"Few-shot Prompting"),(0,a.kt)("p",null,"In some caes, it is easier to show the model an example of the dsired output rather than writing a description. This can be done with faked example messages."),(0,a.kt)("p",null,"Set the name field of the system messages to example_user or example_assistant to ensure the model understands the example messages are not part of a real conversation and should not be referenced again. "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},'# An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\nresponse = openai.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {"role": "system", "content": "You are a helpful, pattern-following assistant."},\n        {"role": "user", "content": "Help me translate the following corporate jargon into plain English."},\n        {"role": "assistant", "content": "Sure, I\'d be happy to!"},\n        {"role": "user", "content": "New synergies will help drive top-line growth."},\n        {"role": "assistant", "content": "Things working well together will increase revenue."},\n        {"role": "user", "content": "Let\'s circle back when we have more bandwidth to touch base on opportunities for increased leverage."},\n        {"role": "assistant", "content": "Let\'s talk later when we\'re less busy about how to do better."},\n        {"role": "user", "content": "This late pivot means we don\'t have time to boil the ocean for the client deliverable."},\n    ],\n    temperature=0,\n)\n\nprint(response["choices"][0]["message"]["content"])\n\n')),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"We don't have enough time to complete everything perfectly for the client.")),(0,a.kt)("hr",null),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},'# The business jargon translation example, but with example names for the example messages\nresponse = openai.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {"role": "system", "content": "You are a helpful, pattern-following assistant that translates corporate jargon into plain English."},\n        {"role": "system", "name":"example_user", "content": "New synergies will help drive top-line growth."},\n        {"role": "system", "name": "example_assistant", "content": "Things working well together will increase revenue."},\n        {"role": "system", "name":"example_user", "content": "Let\'s circle back when we have more bandwidth to touch base on opportunities for increased leverage."},\n        {"role": "system", "name": "example_assistant", "content": "Let\'s talk later when we\'re less busy about how to do better."},\n        {"role": "user", "content": "This late pivot means we don\'t have time to boil the ocean for the client deliverable."},\n    ],\n    temperature=0,\n)\n\nprint(response["choices"][0]["message"]["content"])\n\n')),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"This sudden change in plans means we don't have enough time to do everything for the client's project.")),(0,a.kt)("p",null,'Remember, it can take several attempts at engineering the conversation to achieve the desired results. Be open to experiment with a variety of methods for priming or conditioning the model. For example, one developer found that the user message, "Great job so far, these have been perfect", conditioned the model into providing higher quality responses. '),(0,a.kt)("p",null,"For more information on how to increase model accuracy, see our guide ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md"},"Techniques to improve reliability")," . It was written for nonconversation-based models, but many of the principles can be applied to conversation-based models as well."),(0,a.kt)("h2",{id:"counting-tokens"},"Counting tokens"),(0,a.kt)("p",null,"When you submit your request, the API transforms the messages into a sequence of tokens."),(0,a.kt)("p",null,"The number of tokens used affects:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"the cost of the request"),(0,a.kt)("li",{parentName:"ul"},"the time it takes to generate the response"),(0,a.kt)("li",{parentName:"ul"},"when the reply gets cut off from hitting the maximum token limit (4096 for gpt-3.5-turbo)")),(0,a.kt)("p",null,"As of March 1, 2023, the following fuction will calculate the number of tokens required for the given messages."),(0,a.kt)("h3",{id:"function-for-calculating-tokens"},"Function for Calculating Tokens"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},'import tiktoken\n\n\ndef num_tokens_from_messages(messages, model="gpt-3.5-turbo-0301"):\n    """Returns the number of tokens used by a list of messages."""\n    try:\n        encoding = tiktoken.encoding_for_model(model)\n    except KeyError:\n        encoding = tiktoken.get_encoding("cl100k_base")\n    if model == "gpt-3.5-turbo-0301":  # note: future models may deviate from this\n        num_tokens = 0\n        for message in messages:\n            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n            for key, value in message.items():\n                num_tokens += len(encoding.encode(value))\n                if key == "name":  # if there\'s a name, the role is omitted\n                    num_tokens += -1  # role is always required and always 1 token\n        num_tokens += 2  # every reply is primed with <im_start>assistant\n        return num_tokens\n    else:\n        raise NotImplementedError(f"""num_tokens_from_messages() is not presently implemented for model {model}.\nSee https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.""")\n\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},'messages = [\n    {"role": "system", "content": "You are a helpful, pattern-following assistant that translates corporate jargon into plain English."},\n    {"role": "system", "name":"example_user", "content": "New synergies will help drive top-line growth."},\n    {"role": "system", "name": "example_assistant", "content": "Things working well together will increase revenue."},\n    {"role": "system", "name":"example_user", "content": "Let\'s circle back when we have more bandwidth to touch base on opportunities for increased leverage."},\n    {"role": "system", "name": "example_assistant", "content": "Let\'s talk later when we\'re less busy about how to do better."},\n    {"role": "user", "content": "This late pivot means we don\'t have time to boil the ocean for the client deliverable."},\n]\n\n')),(0,a.kt)("hr",null),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},'# example token count from the function defined above\nprint(f"{num_tokens_from_messages(messages)} prompt tokens counted.")\n\n')),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"126 prompt tokens counted.")),(0,a.kt)("hr",null),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},'# example token count from the OpenAI API\nresponse = openai.ChatCompletion.create(\n    model=MODEL,\n    messages=messages,\n    temperature=0,\n)\n\nprint(f\'{response["usage"]["prompt_tokens"]} prompt tokens used.\')\n\n')),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"126 prompt tokens used.")))}u.isMDXComponent=!0}}]);